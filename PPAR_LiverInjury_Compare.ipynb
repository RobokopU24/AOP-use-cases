{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1475081-371a-4272-bec4-5f43b3579e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter inputs\n",
    "aragorn_submit_url = \"https://robokop-ara.apps.renci.org/robokop/query\"\n",
    "trapi_submit_url = \"http://robokop-automat.apps.renci.org/robokopkg/1.4/query\"\n",
    "\n",
    "automat_cypher_submit_url = \"https://robokop-automat.apps.renci.org/robokopkg/cypher\"\n",
    "robokopkg_bolt_url = \"bolt://robokopkg.renci.org:7687\"\n",
    "\n",
    "input_search_string = 'ppara'\n",
    "output_search_string = 'liver fibrosis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c3766-1053-44e0-826c-87989a39e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=5)\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef544d-5be5-4078-abf6-17037431b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing directory to write\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "write_dir_trapi_automat = Path(\"output/compare/\"+str(dt_string)+\"/trapi/automat\")\n",
    "write_dir_trapi_automat.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "write_dir_trapi_ara = Path(\"output/compare/\"+str(dt_string)+\"/trapi/ara\")\n",
    "write_dir_trapi_ara.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "write_dir_cypher_automat = Path(\"output/compare/\"+str(dt_string)+\"/cypher/automat\")\n",
    "write_dir_cypher_automat.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "write_dir_cypher_robokopkg = Path(\"output/compare/\"+str(dt_string)+\"/cypher/robokopkg\")\n",
    "write_dir_cypher_robokopkg.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d668be-4b73-400d-b679-05ca5ca8d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pathway results to load\n",
    "ARA_results_file = \"output/compare/\"+str(dt_string)+\"/trapi/ara/results_ARA.csv\"\n",
    "TRAPI_results_file = \"output/compare/\"+str(dt_string)+\"/trapi/automat/results_TRAPI.csv\"\n",
    "Cypher_ROBOKOPKG_results_file = \"output/compare/\"+str(dt_string)+\"/cypher/robokopkg/results.csv\"\n",
    "Cypher_Automat_results_file = \"output/compare/\"+str(dt_string)+\"/cypher/automat/results.csv\"\n",
    "write_dir_compare = \"output/compare/\"+str(dt_string)+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaac2ff5-9fde-4773-9917-88de96e4b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Neo4j connection class\n",
    "user = 'neo4j'\n",
    "pw = ''\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "class Neo4jConnection:\n",
    "    \n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__driver = None\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create the driver:\", e)\n",
    "        \n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "        \n",
    "    def query(self, query, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try: \n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session()\n",
    "            response = list(session.run(query))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally: \n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf4345-a469-40b0-946d-94de2274f9de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generating data for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a43a593-ab66-435b-8993-0ff30d31307e",
   "metadata": {},
   "source": [
    "### Getting \"Input/Output\" CURIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cedd0ea-5466-4f38-8c96-8f8dbe7a2d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = requests.post(f'https://name-resolution-sri.renci.org/lookup?string={input_search_string}&offset=0&limit=100')\n",
    "results_json = results.json()\n",
    "\n",
    "input_node_id_list = []\n",
    "for result in results_json:\n",
    "    if result[\"curie\"] not in input_node_id_list:\n",
    "        input_node_id_list.append(result[\"curie\"])\n",
    "print(input_node_id_list)\n",
    "print(f\"Number of 'input' IDs: {len(input_node_id_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25115a90-7977-4f9d-8eb4-21f16b142b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = requests.post(f'https://name-resolution-sri.renci.org/lookup?string={output_search_string}&offset=0&limit=100')\n",
    "results_json = results.json()\n",
    "\n",
    "output_node_id_list = []\n",
    "for result in results_json:\n",
    "    if result[\"curie\"] not in output_node_id_list:\n",
    "        output_node_id_list.append(result[\"curie\"])\n",
    "print(output_node_id_list)\n",
    "print(f\"Number of 'output' IDs: {len(output_node_id_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320b265-28ba-4e01-8603-ebf2522c0a9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TRAPI methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26dcf90-3528-4eff-821e-5a4154a5de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing TRAPI query and extracting results for ARA and Automat\n",
    "query={\n",
    "    \"message\": {\n",
    "      \"query_graph\": {\n",
    "        \"edges\": {\n",
    "          \"e00\": {\n",
    "            \"subject\": \"n00\",\n",
    "              \"object\": \"n01\",\n",
    "          \"predicates\":[\"biolink:related_to\"]\n",
    "          },\n",
    "          \"e01\": {\n",
    "            \"subject\": \"n01\",\n",
    "              \"object\": \"n02\",\n",
    "          \"predicates\":[\"biolink:related_to\"]\n",
    "          }\n",
    "        },\n",
    "        \"nodes\": {\n",
    "          \"n00\": {\n",
    "            \"ids\": input_node_id_list, #['NCBIGene:5465'], #\n",
    "            \"categories\": [\"biolink:GeneOrGeneProduct\"]\n",
    "          },\n",
    "          \"n01\": {\n",
    "              \"categories\": [\"biolink:BiologicalEntity\"]\n",
    "          },\n",
    "          \"n02\": {\n",
    "            \"ids\": output_node_id_list, #[\"HP:0001395\"],\n",
    "            \"categories\": [\"biolink:DiseaseOrPhenotypicFeature\"]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ccd0b-96f0-4364-b5a0-f45f023ce64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_trapi = requests.post(trapi_submit_url,json=query)\n",
    "print(response_trapi.status_code)\n",
    "number_pathway_results_trapi = len(response_trapi.json()['message']['results'])\n",
    "print(len(response_trapi.json()['message']['results']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966647b-c2cc-4398-9544-d4d043c2b1d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kg_trapi = response_trapi.json()['message']['knowledge_graph']\n",
    "results_trapi = response_trapi.json()['message']['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27231687-ccc0-4689-aa09-e891a60f9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "cols = []\n",
    "for node in sorted(results_trapi[0]['node_bindings'].keys()):\n",
    "    cols.append(node)\n",
    "    cols.append(node + '_name')\n",
    "results_trapi_df = pd.DataFrame(columns = cols)\n",
    "\n",
    "results_trapi_list = []\n",
    "for result in results_trapi:\n",
    "    result_dict = {}\n",
    "    for node in sorted(result['node_bindings'].keys()):\n",
    "        node_id = result['node_bindings'][node][0]['id']\n",
    "        result_dict[node] = node_id\n",
    "        result_dict[node + '_name'] = kg_trapi['nodes'][node_id]['name']\n",
    "\n",
    "    results_trapi_list.append(pd.DataFrame([result_dict]))\n",
    "results_trapi_df = pd.concat(results_trapi_list)\n",
    "display(results_trapi_df)\n",
    "results_trapi_df.to_csv(TRAPI_results_file, index=False)\n",
    "\n",
    "combined_node_list = [\"_\".join([row[1].replace(\" \", \"_\"), row[3].replace(\" \", \"_\"), row[5].replace(\" \", \"_\")]) for row in results_trapi_df[cols].to_numpy()]\n",
    "pp.pprint(combined_node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1709b322-6bfb-4dd6-a50a-977c4eba0b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=5)\n",
    "\n",
    "for i in range(number_pathway_results_trapi):\n",
    "    # print(f\"Pathway result: {combined_node_list[i]}\")\n",
    "    edge_bindings = results_trapi[i]['analyses'][0]['edge_bindings']\n",
    "\n",
    "    edge_ids = []\n",
    "    for edge_name, edge_list in edge_bindings.items():\n",
    "        edge_ids.append({edge_name: [x['id'] for x in edge_list]})\n",
    "\n",
    "    string_out_list = []\n",
    "    for edge_dict in edge_ids:\n",
    "        for edge_name, edge_list in edge_dict.items():\n",
    "            for edge_id in edge_list:\n",
    "                source_list_raw = [ sub['attribute_source'] for sub in kg_trapi['edges'][edge_id]['attributes'] ]\n",
    "                source_list = []\n",
    "                for value in source_list_raw:\n",
    "                    if value != None:\n",
    "                        source_list.append(value)\n",
    "                source_list = set(source_list)\n",
    "                subject_id = kg_trapi['edges'][edge_id]['subject']\n",
    "                subject = kg_trapi['nodes'][subject_id]['name']\n",
    "                predicate = kg_trapi['edges'][edge_id]['predicate']\n",
    "                object_id = kg_trapi['edges'][edge_id]['object']\n",
    "                object = kg_trapi['nodes'][object_id]['name']\n",
    "                string_out = f\"{subject} -> {predicate} -> {object}|{source_list}\"\n",
    "                string_out_list.append(string_out)\n",
    "    string_out_dict = dict(Counter(string_out_list).items())\n",
    "    pp.pprint(string_out_dict)\n",
    "    print(\"\")\n",
    "    \n",
    "    with open(os.path.join(write_dir_trapi_automat,combined_node_list[i]+\".txt\"), 'a') as convert_file:\n",
    "        convert_file.write(json.dumps(string_out_dict))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07aa045-64c1-4d68-93aa-bc6587bbbe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_ara = requests.post(aragorn_submit_url,json=query)\n",
    "print(response_ara.status_code)\n",
    "number_pathway_results_ara = len(response_ara.json()['message']['results'])\n",
    "print(len(response_ara.json()['message']['results']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a43d83-eaee-4e8b-9284-a9fa3b3db376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kg_ara = response_ara.json()['message']['knowledge_graph']\n",
    "results_ara = response_ara.json()['message']['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b75f55-d5dc-4695-912e-9907f4c7d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "cols = []\n",
    "for node in sorted(results_ara[0]['node_bindings'].keys()):\n",
    "    cols.append(node)\n",
    "    cols.append(node + '_name')\n",
    "results_ara_df = pd.DataFrame(columns = cols)\n",
    "\n",
    "results_ara_list = []\n",
    "for result in results_ara:\n",
    "    result_dict = {}\n",
    "    for node in sorted(result['node_bindings'].keys()):\n",
    "        node_id = result['node_bindings'][node][0]['id']\n",
    "        result_dict[node] = node_id\n",
    "        result_dict[node + '_name'] = kg_ara['nodes'][node_id]['name']\n",
    "\n",
    "    results_ara_list.append(pd.DataFrame([result_dict]))\n",
    "results_ara_df = pd.concat(results_ara_list)\n",
    "display(results_ara_df)\n",
    "results_ara_df.to_csv(ARA_results_file, index=False)\n",
    "\n",
    "combined_node_list = [\"_\".join([row[1].replace(\" \", \"_\"), row[3].replace(\" \", \"_\"), row[5].replace(\" \", \"_\")]) for row in results_ara_df[cols].to_numpy()]\n",
    "pp.pprint(combined_node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5893f4-61c3-4599-aeaa-f6883cd830ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=5)\n",
    "\n",
    "for i in range(number_pathway_results_ara):\n",
    "    print(f\"Pathway result: {combined_node_list[i]}\")\n",
    "    edge_bindings = results_ara[i]['analyses'][0]['edge_bindings']\n",
    "\n",
    "    edge_ids = []\n",
    "    for edge_name, edge_list in edge_bindings.items():\n",
    "        edge_ids.append({edge_name: [x['id'] for x in edge_list]})\n",
    "\n",
    "    string_out_list = []\n",
    "    for edge_dict in edge_ids:\n",
    "        for edge_name, edge_list in edge_dict.items():\n",
    "            for edge_id in edge_list:\n",
    "                subject_id = kg_ara['edges'][edge_id]['subject']\n",
    "                subject = kg_ara['nodes'][subject_id]['name']\n",
    "                predicate = kg_ara['edges'][edge_id]['predicate']\n",
    "                object_id = kg_ara['edges'][edge_id]['object']\n",
    "                object = kg_ara['nodes'][object_id]['name']\n",
    "                string_out = f\"{subject} -> {predicate} -> {object}\"\n",
    "                string_out_list.append(string_out)\n",
    "    string_out_dict = dict(Counter(string_out_list).items())\n",
    "    pp.pprint(string_out_dict)\n",
    "    print(\"\")\n",
    "    \n",
    "    with open(os.path.join(write_dir_trapi_ara,combined_node_list[i]+\".txt\"), 'a') as convert_file:\n",
    "        convert_file.write(json.dumps(string_out_dict))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b3516-24e6-40f7-9ea9-85a401a1a7f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cypher methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2666cce-0422-4c3a-add1-4200f9b137f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Cypher query and extracting results from Automat using Cypher\n",
    "\n",
    "nodes = ['n00','n01','n02']\n",
    "relationships = ['r0','r1']\n",
    "cols = []\n",
    "for node in nodes:\n",
    "    cols.append(node)\n",
    "    cols.append(node+\"_name\")\n",
    "\n",
    "results_df = pd.DataFrame(columns = cols)\n",
    "results_dict_list = []\n",
    "\n",
    "additional_input_node_ids = ['UniProtKB:Q07869-1', 'UniProtKB:Q3L8U1-2', 'UniProtKB:Q9UBK2-1']\n",
    "additional_output_node_ids = ['HP:0001405', 'HP:0012852']\n",
    "\n",
    "i = 0\n",
    "\n",
    "cypher = f\"\"\"MATCH\n",
    "({nodes[0]}:`biolink:GeneOrGeneProduct`)-[{relationships[0]}]-({nodes[1]}:`biolink:BiologicalEntity`)-[{relationships[1]}]-({nodes[2]}:`biolink:DiseaseOrPhenotypicFeature`)\n",
    "WHERE {nodes[0]}.id IN {input_node_id_list+additional_input_node_ids} AND {nodes[2]}.id IN {output_node_id_list+additional_output_node_ids}\n",
    "RETURN [startNode({relationships[0]}),[type({relationships[0]}),properties({relationships[0]})],endNode({relationships[0]})] as edge_1, \n",
    "[startNode({relationships[1]}),[type({relationships[1]}),properties({relationships[1]})],endNode({relationships[1]})] as edge_2, \n",
    "[{nodes[0]}.name, {nodes[1]}.name, {nodes[2]}.name] as node_names,\n",
    "[{nodes[0]}.id, {nodes[1]}.id, {nodes[2]}.id] as node_ids\"\"\"\n",
    "json_query = {'query': cypher}\n",
    "\n",
    "results = requests.post(automat_cypher_submit_url,json=json_query, timeout=(40,200))\n",
    "results_json = results.json()\n",
    "\n",
    "# pp.pprint(results_json)\n",
    "for result in results_json['results'][0]['data']:\n",
    "    string_out_list = []\n",
    "    for item in result['row'][0:2]:\n",
    "        string_out = f\"{item[0]['name']} -> {item[1][0]} -> {item[2]['name']}||{item[1][1]}\"\n",
    "        string_out_ids = f\"{item[0]['id']} -> {item[1][0]} -> {item[2]['id']}||{item[1][1]['biolink:primary_knowledge_source']}\"\n",
    "        print()\n",
    "        print(string_out_ids)\n",
    "        if string_out not in string_out_list:\n",
    "            string_out_list.append(string_out)\n",
    "    if len(results_json['results'][0]['data']) > 0:\n",
    "        combined_node_list = \"_\".join(results_json['results'][0]['data'][0]['row'][2]).replace(\" \", \"_\")\n",
    "        # print(combined_node_list)\n",
    "\n",
    "    # string_out_list = [i.split('||', 1)[0] for i in string_out_list]\n",
    "\n",
    "    string_out_dict = dict(Counter(string_out_list).items())\n",
    "    if len(string_out_dict.keys()) == 0:\n",
    "        pass\n",
    "        # print(f\"None found for {input_node_id}-{output_node_id}\")\n",
    "    else:\n",
    "        combined_node_list = \"_\".join(result['row'][2]).replace(\" \", \"_\")\n",
    "        print(combined_node_list)\n",
    "\n",
    "        string_out_list = [i.split('||', 1)[0] for i in string_out_list]\n",
    "\n",
    "        string_out_dict = dict(Counter(string_out_list).items())\n",
    "        pp.pprint(string_out_dict)\n",
    "        # i = i + 1\n",
    "\n",
    "        with open(os.path.join(write_dir_cypher_automat,combined_node_list+\".txt\"), 'a') as convert_file:\n",
    "            convert_file.write(json.dumps(string_out_dict))\n",
    "\n",
    "    result_dict = {}\n",
    "    for k in range(len(nodes)):\n",
    "        node_id = result['row'][3][k]\n",
    "        node_name = result['row'][2][k]\n",
    "        result_dict[nodes[k]] = node_id\n",
    "        result_dict[nodes[k]+\"_name\"] = node_name\n",
    "    # pp.pprint(result_dict)\n",
    "\n",
    "    results_dict_list.append(result_dict)\n",
    "\n",
    "results_df = pd.concat([results_df,pd.DataFrame.from_records(results_dict_list)])\n",
    "print(results_df.shape)\n",
    "results_df.to_csv(os.path.join(write_dir_cypher_automat,'results.csv'))\n",
    "results_df.to_csv(Cypher_Automat_results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a812e7-e26d-4405-9ce9-ec4c129704e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Initializing Cypher query and extracting results from ROBOKOPKG using Bolt protocol method\n",
    "conn = Neo4jConnection(uri=robokopkg_bolt_url, user = user, pwd = pw)\n",
    "\n",
    "nodes = ['n00','n01','n02']\n",
    "relationships = ['r0','r1']\n",
    "\n",
    "cols = []\n",
    "for node in nodes:\n",
    "    cols.append(node)\n",
    "    cols.append(node+\"_name\")\n",
    "\n",
    "results_df = pd.DataFrame(columns = cols)\n",
    "results_dict_list = []\n",
    "\n",
    "additional_input_node_ids = ['UniProtKB:Q07869-1', 'UniProtKB:Q3L8U1-2', 'UniProtKB:Q9UBK2-1']\n",
    "additional_output_node_ids = ['HP:0001405', 'HP:0012852']\n",
    "\n",
    "cypher = f\"\"\"MATCH\n",
    "({nodes[0]}:`biolink:GeneOrGeneProduct`)-[{relationships[0]}]-({nodes[1]}:`biolink:BiologicalEntity`)-[{relationships[1]}]-({nodes[2]}:`biolink:DiseaseOrPhenotypicFeature`)\n",
    "WHERE {nodes[0]}.id IN {input_node_id_list+additional_input_node_ids} AND {nodes[2]}.id IN {output_node_id_list+additional_output_node_ids}\n",
    "RETURN [startNode({relationships[0]}),[type({relationships[0]}),properties({relationships[0]})],endNode({relationships[0]})] as edge_1, \n",
    "[startNode({relationships[1]}),[type({relationships[1]}),properties({relationships[1]})],endNode({relationships[1]})] as edge_2, \n",
    "[{nodes[0]}.name, {nodes[1]}.name, {nodes[2]}.name] as node_names,\n",
    "[{nodes[0]}.id, {nodes[1]}.id, {nodes[2]}.id] as node_ids\"\"\"\n",
    "\n",
    "record_list = conn.query(cypher)\n",
    "\n",
    "i = 0\n",
    "print(f\"Number of records returned: {len(record_list)}\")\n",
    "\n",
    "results_df = pd.DataFrame(columns = cols)\n",
    "results_dict_list = []\n",
    "\n",
    "for record in record_list:\n",
    "    string_out_list = []\n",
    "    record_data = record.data()\n",
    "    #only grab the edge information and skip the node names and IDs\n",
    "    record_data_first2 = {k: record_data[k] for k in list(record_data)[:2]}\n",
    "    for label, data in record_data_first2.items():\n",
    "        string_out = f\"{label} - {data[0]['name']} -> {data[1][0]} -> {data[2]['name']}||{data[1][1]}\"\n",
    "        string_out_ids = f\"{label} - {data[0]['id']} -> {data[1][0]} -> {data[2]['id']}||{data[1][1]['biolink:primary_knowledge_source']}\"\n",
    "        print(string_out_ids)\n",
    "        if string_out not in string_out_list:\n",
    "            string_out_list.append(string_out)\n",
    "\n",
    "    if len(record_list) > 0:\n",
    "        combined_node_list = \"_\".join(list(record.data('node_names').values())[0])\n",
    "        print(combined_node_list)\n",
    "\n",
    "        string_out_list = [i.split('||', 1)[0] for i in string_out_list]\n",
    "\n",
    "        string_out_dict = dict(Counter(string_out_list).items())\n",
    "        pp.pprint(string_out_dict)\n",
    "\n",
    "        with open(os.path.join(write_dir_cypher_robokopkg,combined_node_list+\".txt\"), 'a') as convert_file:\n",
    "            convert_file.write(json.dumps(string_out_dict))\n",
    "\n",
    "    record_data = record.data()\n",
    "    #only grab the node labels and IDs\n",
    "    record_data_last2 = {k: record_data[k] for k in list(record_data)[2:]}\n",
    "\n",
    "    result_dict = {}\n",
    "    for label, data in record_data_last2.items():\n",
    "        # print(f\"{label}: {data}\")\n",
    "        for j in range(len(nodes)):\n",
    "            if \"ids\" in label:\n",
    "                node_id = data[j]\n",
    "                result_dict[nodes[j]] = node_id\n",
    "            elif \"names\" in label:\n",
    "                node_name = data[j]\n",
    "                result_dict[nodes[j]+\"_name\"] = node_name\n",
    "    # pp.pprint(result_dict)\n",
    "\n",
    "    results_dict_list.append(result_dict)\n",
    "\n",
    "results_df = pd.concat([results_df,pd.DataFrame.from_records(results_dict_list)])\n",
    "print(results_df.shape)\n",
    "display(results_df)\n",
    "results_df.to_csv(Cypher_ROBOKOPKG_results_file)\n",
    "conn.close()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2eff9c-73f0-4a24-a4e7-d1bb242fda0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Summary comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1422ba0-beee-4193-9bd9-ae9c38631986",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initializing and defining summary compare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f301ed-cae5-4ef2-95e2-2dcb2b2256de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "def get_summary_stats(results_df):\n",
    "    results_df['Freq'] = 1\n",
    "\n",
    "    occur_n00=(results_df\n",
    "      .groupby(\n",
    "           results_df[[\"n00\"]]\n",
    "           .apply(lambda x: str(sorted(x)), axis=1)\n",
    "           )\n",
    "      .agg({\"n00\": \"first\", \"n00_name\": \"first\", \"Freq\": \"sum\"}).sort_values(['Freq'], ascending = False)\n",
    "      .reset_index(drop=True)\n",
    "    )\n",
    "    occur_n02=(results_df\n",
    "      .groupby(\n",
    "           results_df[[\"n02\"]]\n",
    "           .apply(lambda x: str(sorted(x)), axis=1)\n",
    "           )\n",
    "      .agg({\"n02\": \"first\",\"n02_name\": \"first\", \"Freq\": \"sum\"}).sort_values(['Freq'], ascending = False)\n",
    "      .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Counting pairwise results\n",
    "    occur_pair=(results_df\n",
    "      .groupby(\n",
    "           results_df[[\"n00\", \"n02\"]]\n",
    "           .apply(lambda x: str(sorted(x)), axis=1)\n",
    "           )\n",
    "      .agg({\"n00\": \"first\", \"n00_name\": \"first\", \"n02\": \"first\", \"n02_name\": \"first\", \"Freq\": \"sum\"}).sort_values(['Freq'], ascending = False)\n",
    "      .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(\"\\nFrequency of n00 IDs\")\n",
    "    display(occur_n00)\n",
    "    print(\"\\nFrequency of n02 IDs\")\n",
    "    display(occur_n02)\n",
    "    print(\"\\nFrequency of n00-n02 pair IDs\")\n",
    "    display(occur_pair)\n",
    "    \n",
    "    print(\"\\nMost frequent pair\")\n",
    "    display(occur_pair.head(1))\n",
    "    return(occur_pair.head(1))\n",
    "    \n",
    "def get_summary_stats_no_ids(results_df):\n",
    "    results_df['Freq'] = 1\n",
    "\n",
    "    occur_n00=(results_df\n",
    "      .groupby(\n",
    "           results_df[[\"n00_name\"]]\n",
    "           .apply(lambda x: str(sorted(x)), axis=1)\n",
    "           )\n",
    "      .agg({\"n00_name\": \"first\", \"Freq\": \"sum\"}).sort_values(['Freq'], ascending = False)\n",
    "      .reset_index(drop=True)\n",
    "    )\n",
    "    occur_n02=(results_df\n",
    "      .groupby(\n",
    "           results_df[[\"n02_name\"]]\n",
    "           .apply(lambda x: str(sorted(x)), axis=1)\n",
    "           )\n",
    "      .agg({\"n02_name\": \"first\", \"Freq\": \"sum\"}).sort_values(['Freq'], ascending = False)\n",
    "      .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Counting pairwise results\n",
    "    occur_pair=(results_df\n",
    "      .groupby(\n",
    "           results_df[[\"n00_name\", \"n02_name\"]]\n",
    "           .apply(lambda x: str(sorted(x)), axis=1)\n",
    "           )\n",
    "      .agg({\"n00_name\": \"first\", \"n02_name\": \"first\", \"Freq\": \"sum\"}).sort_values(['Freq'], ascending = False)\n",
    "      .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(\"\\nFrequency of n00 labels\")\n",
    "    display(occur_n00)\n",
    "    print(\"\\nFrequency of n02 labels\")\n",
    "    display(occur_n02)\n",
    "    print(\"\\nFrequency of n00-n02 pair labels\")\n",
    "    display(occur_pair)\n",
    "    \n",
    "    print(\"\\nMost frequent pair\")\n",
    "    display(occur_pair.head(1))\n",
    "    return(occur_pair.head(1))\n",
    "    \n",
    "def compare_summary_stats(results_df1, results_df2, label1, label2):\n",
    "    print(f\"Summary stats for {label1}\")\n",
    "    most_frequent_pair1 = get_summary_stats(results_df1)\n",
    "    most_frequent_pair1['label'] = label1\n",
    "    print(f\"\\nSummary stats for {label2}\")\n",
    "    most_frequent_pair2 = get_summary_stats(results_df2)\n",
    "    most_frequent_pair2['label'] = label2\n",
    "    frames = [most_frequent_pair1,most_frequent_pair2]\n",
    "    most_frequent_pairs = pd.concat(frames)\n",
    "    display(most_frequent_pairs)\n",
    "    \n",
    "def compare_summary_stats_no_ids(results_df1, results_df2, label1, label2):\n",
    "    print(f\"Summary stats for {label1}\")\n",
    "    most_frequent_pair1 = get_summary_stats_no_ids(results_df1)\n",
    "    most_frequent_pair1['label'] = label1\n",
    "    print(f\"\\nSummary stats for {label2}\")\n",
    "    most_frequent_pair2 = get_summary_stats_no_ids(results_df2)\n",
    "    most_frequent_pair2['label'] = label2\n",
    "    frames = [most_frequent_pair1,most_frequent_pair2]\n",
    "    most_frequent_pairs = pd.concat(frames)\n",
    "    display(most_frequent_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581811b-81dc-435c-a968-c7b5bb76f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_Cypher_Automat = pd.read_csv(Cypher_Automat_results_file)\n",
    "display(results_df_Cypher_Automat)\n",
    "most_common_pair_test = get_summary_stats(results_df_Cypher_Automat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d570812b-23ca-4f7b-b048-1d22c869ea0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TRAPI comparison results\n",
    "ARA vs TRAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a24fe1-c552-4c47-bdcc-797aba0f160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_ARA = pd.read_csv(ARA_results_file)\n",
    "results_df_TRAPI = pd.read_csv(TRAPI_results_file)\n",
    "compare_summary_stats(results_df_ARA,results_df_TRAPI,\"ARA\",\"TRAPI (automat)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eae466-12e4-4423-afcb-41cccfb87fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_ARA = pd.read_csv(ARA_results_file)\n",
    "results_df_Cypher_ROBOKOPKG = pd.read_csv(Cypher_ROBOKOPKG_results_file)\n",
    "compare_summary_stats(results_df_ARA,results_df_Cypher_ROBOKOPKG,\"ARA\",\"Cypher (ROBOKOPKG)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade213e0-0fd4-4af5-8fbe-4158988417d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cypher comparison results\n",
    "Automat vs ROBOKOPKG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a35620d-e447-4497-b048-52594e34e0d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ARA vs Cypher (ROBOKOPKG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617e836b-ebc7-46f5-876d-f2745f68a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_ARA = pd.read_csv(ARA_results_file)\n",
    "results_df_Cypher_ROBOKOPKG = pd.read_csv(Cypher_ROBOKOPKG_results_file)\n",
    "compare_summary_stats(results_df_ARA,results_df_Cypher_ROBOKOPKG,\"ARA\",\"Cypher (ROBOKOPKG)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c94a593-7086-4b57-a7c2-627f6460c989",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Automat comparison results\n",
    "TRAPI vs Cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f06878e-aab8-41f2-8ca4-8890c67c816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_TRAPI = pd.read_csv(TRAPI_results_file)\n",
    "results_df_Cypher_Automat = pd.read_csv(Cypher_Automat_results_file)\n",
    "compare_summary_stats(results_df_TRAPI,results_df_Cypher_Automat,\"TRAPI (automat)\",\"Cypher (automat)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac221255-e4cc-4c25-9145-5771731feb92",
   "metadata": {},
   "source": [
    "# Comparison plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a8f3d8-3bae-4144-a6b7-2defafe1bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and defining pandas dataframes with results\n",
    "# If a data file isn't found, an indication will be printed saying so\n",
    "if Path.exists(Path((ARA_results_file))):\n",
    "    results_df_ARA = pd.read_csv(ARA_results_file)\n",
    "else:\n",
    "    print(\"ARA not found\")\n",
    "if Path.exists(Path((TRAPI_results_file))):\n",
    "    results_df_TRAPI = pd.read_csv(TRAPI_results_file)\n",
    "else:\n",
    "    print(\"TRAPI not found\")\n",
    "if Path.exists(Path((Cypher_Automat_results_file))):\n",
    "    results_df_Cypher_Automat = pd.read_csv(Cypher_Automat_results_file)\n",
    "else:\n",
    "    print(\"Cypher (Automat) not found\")\n",
    "if Path.exists(Path((Cypher_ROBOKOPKG_results_file))):\n",
    "    results_df_Cypher_ROBOKOPKG = pd.read_csv(Cypher_ROBOKOPKG_results_file)\n",
    "else:\n",
    "    print(\"Cypher (ROBOKOP) not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd401da-803b-4b3f-8351-2dce28f3a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library and defining compare_results() function for later demo use\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "def convert_node_list_to_df(input_list):\n",
    "    nodes = ['n00','n01','n02']\n",
    "    results_df = pd.DataFrame(columns = nodes)\n",
    "    \n",
    "    dict_list = []\n",
    "    for item_combined in input_list:\n",
    "        item_list = item_combined.split(\"|\")\n",
    "        dict_to_add = {}\n",
    "        for i in range(len(nodes)):\n",
    "            dict_to_add[nodes[i]] = item_list[i]\n",
    "        dict_list.append(dict_to_add)\n",
    "        \n",
    "    results_df = pd.concat([results_df,pd.DataFrame.from_records(dict_list)])\n",
    "    display(results_df)\n",
    "\n",
    "def compare_results(df1, df2, output_file_name, label1, label2):\n",
    "    # This function takes the results of two dictionaries created by the TRAPI and Cypher query Jupyter notebooks and gets the following:\n",
    "    #  -finds how many edges each set has in common, including having the same and different number of occurrences\n",
    "    #  -finds edges exclusive to each set\n",
    "    #  -plots a venn diagram representation of common edges and differences between sets\n",
    "    #    -edges with the same predicate but different occurrences are treated as differences in the diagram\n",
    "    # INPUT:\n",
    "    #  -df1: dictionary with keys in the format of \"node_label_1 -> predicate -> node_label_2\" and corresponding values being occurrences within that set\n",
    "    #  -df2: dictionary with keys in the format of \"node_label_1 -> predicate -> node_label_2\" and corresponding values being occurrences within that set\n",
    "    #  -output_file_name: name of the file to write to with output results\n",
    "    #  -label1/label2: Text and figure label for first and second input dictionaries, respectively\n",
    "    # OUTPUT:\n",
    "    #  -compare .txt file listing each of the following:\n",
    "    #    -Common edges\n",
    "    #    -Common edge predicates with different counts\n",
    "    #    -Edges exclusive to the first input dictionary\n",
    "    #    -Edges exclusive to the second input dictionary\n",
    "    #  -Venn diagram showing number of common and different edges between the two input sets\n",
    "    both_have_ids = False\n",
    "    df1_has_ids = any(item in ['n00','n01','n02'] for item in df1.columns)\n",
    "    df2_has_ids = any(item in ['n00','n01','n02'] for item in df2.columns)\n",
    "    \n",
    "    if df1_has_ids and df2_has_ids:\n",
    "        both_have_ids = True\n",
    "    \n",
    "    if both_have_ids:\n",
    "        col1 = df1.apply(lambda row: '|'.join([row['n00'],row['n01'],row['n02']]), axis=1)\n",
    "        col2 = df2.apply(lambda row: '|'.join([row['n00'],row['n01'],row['n02']]), axis=1)\n",
    "    else:\n",
    "        col1 = df1.apply(lambda row: '|'.join([row['n00_name'],row['n01_name'],row['n02_name']]), axis=1)\n",
    "        col2 = df2.apply(lambda row: '|'.join([row['n00_name'],row['n01_name'],row['n02_name']]), axis=1)\n",
    "\n",
    "    pathways_in_common_list = list(set(col1) & set(col2))\n",
    "    pathways_col1_list = list(set(col1) - set(col2))\n",
    "    pathways_col2_list = list(set(col2) - set(col1))\n",
    "    \n",
    "    print(\"Node pathways in common:\\n\")\n",
    "    convert_node_list_to_df(pathways_in_common_list)\n",
    "    print(f\"\\nNode pathways in {label1} only:\\n\")\n",
    "    convert_node_list_to_df(pathways_col1_list)\n",
    "    print(f\"\\nNode pathways in {label2} only:\\n\")\n",
    "    convert_node_list_to_df(pathways_col2_list)\n",
    "    \n",
    "    with open(output_file_name, \"w\") as f:\n",
    "        f.write(\"Common pathways:\\n\")\n",
    "        for pathway in pathways_in_common_list:\n",
    "            f.write(pathway)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(f\"\\n{label1} only:\\n\")\n",
    "        for pathway in pathways_col1_list:\n",
    "            f.write(pathway)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(f\"\\n{label2} only:\\n\")\n",
    "        for pathway in pathways_col2_list:\n",
    "            f.write(pathway)\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    venn2(subsets = (len(pathways_col1_list), len(pathways_col2_list), len(pathways_in_common_list)), set_labels = (label1, label2))\n",
    "    plt.savefig(output_file_name.replace(\".txt\",\"_venn.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    col3 = col1.value_counts()\n",
    "    col4 = col2.value_counts()\n",
    "        \n",
    "    df_merge_input = pd.DataFrame([col3,col4]).T\n",
    "    df_merge_input.columns = [label1,label2]\n",
    "    df_merge_input[label1][df_merge_input[label1].isnull()] = 0\n",
    "    df_merge_input[label2][df_merge_input[label2].isnull()] = 0\n",
    "    df_merge_input_label1 = df_merge_input[(df_merge_input[label1] > 0) & (df_merge_input[label2] == 0)]\n",
    "    df_merge_input_label2 = df_merge_input[(df_merge_input[label1] == 0) & (df_merge_input[label2] > 0)]\n",
    "    df_merge_input_common = df_merge_input[(df_merge_input[label1] > 0) & (df_merge_input[label2] > 0)]\n",
    "    df_merge_input_common['set_difference'] = df_merge_input_common[label2] - df_merge_input_common[label1]\n",
    "    df_merge_input_diff = df_merge_input[(df_merge_input[label1] == 0) | (df_merge_input[label2] == 0)]\n",
    "    df_merge_input_diff['set_difference'] = df_merge_input_diff[label2] - df_merge_input_diff[label1]\n",
    "    print(f\"Number of common pathways: {len(df_merge_input_common['set_difference'])}\")\n",
    "    print(f\"Number pathways in {label1} alone: {len(df_merge_input_label1[label1])}\")\n",
    "    print(f\"Number pathways in {label2} alone: {len(df_merge_input_label2[label2])}\")\n",
    "    # display(df_merge_input_common)\n",
    "    display(df_merge_input_label1)\n",
    "    display(df_merge_input_label2)\n",
    "\n",
    "    y = df_merge_input_diff['set_difference']\n",
    "    x = df_merge_input_diff.index.values.tolist()\n",
    "    plt.figure()\n",
    "    plt.barh(x,y)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(f'{label1} vs {label2}')\n",
    "    max_set_difference = max(df_merge_input_diff['set_difference'])\n",
    "    min_set_difference = min(df_merge_input_diff['set_difference'])\n",
    "    plt.xlim([min(-3,min_set_difference),max(3,max_set_difference)])\n",
    "    plt.savefig(output_file_name.replace(\".txt\",\"_diffplot.png\"), bbox_inches=\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641bfd9e-0425-4285-83d4-317de59b9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ca2e0-45f7-496c-ac8b-901398af3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARA and Cypher (robokopkg) sets\n",
    "output_file_name = os.path.join(write_dir_compare,\"compare_ara_vs_cypher-robokopkg.txt\")\n",
    "compare_results(results_df_ARA,results_df_Cypher_ROBOKOPKG, output_file_name, \"ARA\", \"Cypher (ROBOKOPKG)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f467cd7-ca65-49cb-91e5-f42cece1ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARA and Cypher (automat) sets\n",
    "output_file_name = os.path.join(write_dir_compare,\"compare_ara_vs_cypher-automat.txt\")\n",
    "compare_results(results_df_ARA,results_df_Cypher_Automat, output_file_name, \"ARA\", \"Cypher (Automat)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fba9ff-1f66-433b-9646-2a0ad1c41812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARA and TRAPI\n",
    "output_file_name = os.path.join(write_dir_compare,\"compare_ara_vs_trapi.txt\")\n",
    "compare_results(results_df_ARA,results_df_TRAPI, output_file_name, \"ARA\", \"TRAPI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6966c4d-b729-444d-98d5-b06c77236647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cypher (automat) and Cypher (robokopkg) sets\n",
    "output_file_name = os.path.join(write_dir_compare,\"compare_cypher-automat_vs_cypher-robokopkg.txt\")\n",
    "compare_results(results_df_Cypher_Automat,results_df_Cypher_ROBOKOPKG, output_file_name, \"Cypher (Automat)\", \"Cypher (ROBOKOPKG)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e2f1d-21e8-4eac-82ba-3b3c700cbe9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
